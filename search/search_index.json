{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Documentation Full Code Github Link Overview Everyday a lot of accidents in the world happen due to the recklessness of drivers. Getting details immediately about the parties involved in the accident manually takes time. Hence, we propose using CCTV cameras and applying computer vision techniques in order to monitor traffic situations. Previous attempts at this have been restricted to one or two aspects of rule-breaking. We want to build a unified platform which will detect cars which don\u2019t adhere to a set of rule-breaking aspects. Goals Building a highly scalable web application with features to upload video or embed live stream directly to the application. Once the user uploads a video/opens the live stream to the platform, he can see the output video stream on the application. The application will also capture photos of vehicles breaking following rules and store them in a database for future analysis. The app will be built such that it will be flexible to the addition of various other rules if required as well. The following scenarios will be taken care of by the app implemented in the project: Vehicles should stop for pedestrians. Vehicles should stop if the signal is red. Vehicles should be under the speed limit. Crash detection. Project layout backend/ # Flask application source code frontend/ # Angular application source code examples/ # Test videos with different features docker-compose.yml # Yaml configuration file for docker compose README.md docs/ mkdocs.yml # Documentation Configuration File Project Anstract.pdf # Project Abstract Team1_design_doc.pdf # Design Document ... # UI layouts, other images, etc. docs/ index.md # The documentation homepage. ... # Other markdown files for documentation.","title":"Getting Started"},{"location":"#welcome-to-documentation","text":"Full Code Github Link","title":"Welcome to Documentation"},{"location":"#overview","text":"Everyday a lot of accidents in the world happen due to the recklessness of drivers. Getting details immediately about the parties involved in the accident manually takes time. Hence, we propose using CCTV cameras and applying computer vision techniques in order to monitor traffic situations. Previous attempts at this have been restricted to one or two aspects of rule-breaking. We want to build a unified platform which will detect cars which don\u2019t adhere to a set of rule-breaking aspects.","title":"Overview"},{"location":"#goals","text":"Building a highly scalable web application with features to upload video or embed live stream directly to the application. Once the user uploads a video/opens the live stream to the platform, he can see the output video stream on the application. The application will also capture photos of vehicles breaking following rules and store them in a database for future analysis. The app will be built such that it will be flexible to the addition of various other rules if required as well. The following scenarios will be taken care of by the app implemented in the project: Vehicles should stop for pedestrians. Vehicles should stop if the signal is red. Vehicles should be under the speed limit. Crash detection.","title":"Goals"},{"location":"#project-layout","text":"backend/ # Flask application source code frontend/ # Angular application source code examples/ # Test videos with different features docker-compose.yml # Yaml configuration file for docker compose README.md docs/ mkdocs.yml # Documentation Configuration File Project Anstract.pdf # Project Abstract Team1_design_doc.pdf # Design Document ... # UI layouts, other images, etc. docs/ index.md # The documentation homepage. ... # Other markdown files for documentation.","title":"Project layout"},{"location":"backend/","text":"Backend Server Overview Flask is used as the backend server for creating APIs and processing videos on backend. APIs /api : Type : GET, POST Input : None Purpose : Render flask home Output : Flask home HTML /api/postvideo : Type : POST Input : Video File Purpose : To save video file recieved from front end Output : Success/Error Status /api/getframe/filename : Type : GET Input : Video file name Purpose : To send frame of video to frontend Output : Frame of vide /api/postparameters : Type : POST Input : Parameters corresponding to different violation principles Purpose : To send parameters to backend which will be used in processing Output : Success/Error Status /api/imageslist/feature : Type : GET Input : Featue Type Purpose : Depending on feature selected, return list of images corresponding to that feature Output : List of images which can later be retrieved by frontend /api/getimage : Type : Get Input : None Purpose : Return a sample iamge Output : Sample Image /api/getvideo/feature : Type : GET Input : Feature Type Purpose : Get partial response of video in chunks Output : Partial Response Functions cleanMedia() : Input : None Purpose : To clean all the media files from previous processing Output : None createDirectories() : Input : None Purpose : To create feature directories for runtime purposes Output : None Modules video_stream In house implemented video streaming server. Instead of returning entire video at once, it is returned in chunks which are rendered on browser cv2 : Opencv helps in retrieving frame from a video which user can draw on","title":"Backend Server"},{"location":"backend/#backend-server","text":"","title":"Backend Server"},{"location":"backend/#overview","text":"Flask is used as the backend server for creating APIs and processing videos on backend.","title":"Overview"},{"location":"backend/#apis","text":"/api : Type : GET, POST Input : None Purpose : Render flask home Output : Flask home HTML /api/postvideo : Type : POST Input : Video File Purpose : To save video file recieved from front end Output : Success/Error Status /api/getframe/filename : Type : GET Input : Video file name Purpose : To send frame of video to frontend Output : Frame of vide /api/postparameters : Type : POST Input : Parameters corresponding to different violation principles Purpose : To send parameters to backend which will be used in processing Output : Success/Error Status /api/imageslist/feature : Type : GET Input : Featue Type Purpose : Depending on feature selected, return list of images corresponding to that feature Output : List of images which can later be retrieved by frontend /api/getimage : Type : Get Input : None Purpose : Return a sample iamge Output : Sample Image /api/getvideo/feature : Type : GET Input : Feature Type Purpose : Get partial response of video in chunks Output : Partial Response","title":"APIs"},{"location":"backend/#functions","text":"cleanMedia() : Input : None Purpose : To clean all the media files from previous processing Output : None createDirectories() : Input : None Purpose : To create feature directories for runtime purposes Output : None","title":"Functions"},{"location":"backend/#modules","text":"video_stream In house implemented video streaming server. Instead of returning entire video at once, it is returned in chunks which are rendered on browser cv2 : Opencv helps in retrieving frame from a video which user can draw on","title":"Modules"},{"location":"build/","text":"Building the application Clone/Fork repository First fork or clone this repo: e.g. git clone https://github.com/Sumaid/smart_traffic_analysis.git Build images and run containers with docker-compose Install Docker Desktop . After cloning the repository go inside the project folder: cd smart_traffic_analysis Run docker-compose up which will start a Flask web application for the backend API (default port 8081 ) and an Angular frontend served through a webpack development web server (default port 4200 ). Few things to keep in mind: 1. If you want to run docker-compose in background use -b flag. 2. If your one part crashes like if backend crashes, then you can simply fix the issue and run docker-compose restart backend in a different terminal to get that part running. Similar if frontend crashes, just run docker-compose restart frontend . Access your app In your browser navigate to: http://localhost:4200 (or whatever port you defined for the frontend in docker-compose.yml ). For testing your backend API I recommend using Postman or Curl, but you can also go to http://localhost:8081/api ( if you are just testing simple GET requests ). Working without docker I highly recommend the use of docker and docker-compose as it is far simpler to get started. The great thing about docker is you can run one command docker-compose up on any operating system & environment and your application will be up and running! But still if you want to work without docker, you can go through following steps: Backend development Navigate inside the backend directory: cd backend Install pip dependencies: pip3 install -r requirements.txt Run python3 app.py in backend root (will watch files and restart server on port 8081 on change). Frontend development Navigate inside the frontend directory: cd frontend Assure you have Nodejs installed. Run following commands: rm -rf node_modules dist tmp npm install --save-dev angular-cli@latest npm install npm init Run ng serve --host 0.0.0.0 --disableHostCheck --proxy-config proxy.conf.json in frontend root (will watch files and restart dev-server on port 4200 on change). All calls made to /api will be proxied to backend server (default port for backend 8081 ), this can be changed in proxy.conf.json . Documentation development Run following commands in root directory of project: pip3 install mkdocs mkdocs-material mkdocs serve Documentation will be hosted locally at http://0.0.0.0:8000 . Host can be changed in mkdocs.yml . Documentation is also publicly hosted at https://sumaid.github.io/smart_traffic_analysis/ . If you make any changes in documentation, test it with mkdocs serve then Push changes to remote repository Run mkdocs gh-deploy to deploy changes to publicly hosted documentation Things to keep in mind You can change port numbers in docker-compose.yml , but if you change backend port number, keep in mind to change it in frontend/proxy.conf.dev.json also. If you add new requirements, rerun the command docker-compose up . Each api route in backend should have base /api/ , so create apis in the format, /api/yourroute/ . For any new feature, create a new branch & PR before merging it into master.","title":"Building the application"},{"location":"build/#building-the-application","text":"","title":"Building the application"},{"location":"build/#clonefork-repository","text":"First fork or clone this repo: e.g. git clone https://github.com/Sumaid/smart_traffic_analysis.git","title":"Clone/Fork repository"},{"location":"build/#build-images-and-run-containers-with-docker-compose","text":"Install Docker Desktop . After cloning the repository go inside the project folder: cd smart_traffic_analysis Run docker-compose up which will start a Flask web application for the backend API (default port 8081 ) and an Angular frontend served through a webpack development web server (default port 4200 ). Few things to keep in mind: 1. If you want to run docker-compose in background use -b flag. 2. If your one part crashes like if backend crashes, then you can simply fix the issue and run docker-compose restart backend in a different terminal to get that part running. Similar if frontend crashes, just run docker-compose restart frontend .","title":"Build images and run containers with docker-compose"},{"location":"build/#access-your-app","text":"In your browser navigate to: http://localhost:4200 (or whatever port you defined for the frontend in docker-compose.yml ). For testing your backend API I recommend using Postman or Curl, but you can also go to http://localhost:8081/api ( if you are just testing simple GET requests ).","title":"Access your app"},{"location":"build/#working-without-docker","text":"I highly recommend the use of docker and docker-compose as it is far simpler to get started. The great thing about docker is you can run one command docker-compose up on any operating system & environment and your application will be up and running! But still if you want to work without docker, you can go through following steps:","title":"Working without docker"},{"location":"build/#backend-development","text":"Navigate inside the backend directory: cd backend Install pip dependencies: pip3 install -r requirements.txt Run python3 app.py in backend root (will watch files and restart server on port 8081 on change).","title":"Backend development"},{"location":"build/#frontend-development","text":"Navigate inside the frontend directory: cd frontend Assure you have Nodejs installed. Run following commands: rm -rf node_modules dist tmp npm install --save-dev angular-cli@latest npm install npm init Run ng serve --host 0.0.0.0 --disableHostCheck --proxy-config proxy.conf.json in frontend root (will watch files and restart dev-server on port 4200 on change). All calls made to /api will be proxied to backend server (default port for backend 8081 ), this can be changed in proxy.conf.json .","title":"Frontend development"},{"location":"build/#documentation-development","text":"Run following commands in root directory of project: pip3 install mkdocs mkdocs-material mkdocs serve Documentation will be hosted locally at http://0.0.0.0:8000 . Host can be changed in mkdocs.yml . Documentation is also publicly hosted at https://sumaid.github.io/smart_traffic_analysis/ . If you make any changes in documentation, test it with mkdocs serve then Push changes to remote repository Run mkdocs gh-deploy to deploy changes to publicly hosted documentation","title":"Documentation development"},{"location":"build/#things-to-keep-in-mind","text":"You can change port numbers in docker-compose.yml , but if you change backend port number, keep in mind to change it in frontend/proxy.conf.dev.json also. If you add new requirements, rerun the command docker-compose up . Each api route in backend should have base /api/ , so create apis in the format, /api/yourroute/ . For any new feature, create a new branch & PR before merging it into master.","title":"Things to keep in mind"},{"location":"contributors/","text":"Team Members Sumaid Roll No : 20171092 Email Id : sumaid.ali@students.iiit.ac.in","title":"Contributors"},{"location":"contributors/#team-members","text":"","title":"Team Members"},{"location":"contributors/#sumaid","text":"Roll No : 20171092 Email Id : sumaid.ali@students.iiit.ac.in","title":"Sumaid"},{"location":"design/","text":"Requirements Automatically detect the traffic violations of rules being considered: Compliance with traffic lights Not stopping while pedestrians cross the road Compliance with speed limit Detection of a crash Violation detection should be done through the traffic data that is already available From Video data streamed from cameras, or pre-recorded data fed by user Real-time traffic signal data, that is available in an existing server. Traffic violation detection should happen automatically for all selected junctions and snapshots are to be stored in a database server. There should be a junction/camera selection which can be configured by the traffic police/ concerned authorities. Traffic violation detection should be available for monitoring by the client. They should be able to choose a particular junction/camera feed to monitor Client should be able to feed in custom video data into the system Live processed output should be available if the user demands for the same (GUI to be built for the same) Traffic violations/car crashes should be alarmed on the monitoring displays in the control room. Assumptions Availability of a camera stream/pre recorded data and live streams are connected to the police server. (NOTE: For this project, we will be implementing a Minimum Viable Product(MVP) only the video upload and processing, since we don\u2019t have enough hardware and time to learn and implement the product for a CCTV scenario) In the case that the vehicle details should be detected, we assume that it is done by a separate system which performs automatic number plate detection, and extracts vehicle registration data from an already existing server (this is not in our project scope). Traffic light data is available from all the junctions required as real-time values. Design Rationale The design is based on a distributed architecture, since traffic violation detection is a resource demanding application; Not because of the algorithms involved, but due to the volume of data to be processed. The amount of data processed depends on the following factors: Traffic camera/junctions to be monitored simultaneously. The amount of data processed depends on the day, time of the day etc., due to various traffic behaviors. Because of this, we need a system that automatically scales up as well as down as per the requirement. Also, the system should support live monitoring by a number of officers (clients), which may also increase due to certain events happening in the city. Considering all these the distributed architecture is selected. When we go for distributed architecture, the next question is: \u201cWhat is the basic architecture like?\u201d. The nodes are not geographically distributed, and the network between the nodes are high speed LAN and are very reliable. Hence, the network partition is not considered. So, only node failure is considered. Hence we need a CA system. Architecture Overall High Level Architecture Software Architecture Traffic Violation Engine Architecture Use Case Diagram Microservices Architecture Instead of having a monolithic architecture with layers stacked on one another, we are designing our application based on microservices architecture. Monolithic architecture only allows for vertical scaling, but with microservices architecture it's possible to achieve horizontal scaling easily. Numerous benefits of Microservices architecture include : Modularity : This makes the application easier to understand, develop, test, and become more resilient to architecture erosion. Scalability : Since microservices are implemented and deployed independently of each other, i.e. they run within independent processes, they can be monitored and scaled independently. Distributed development: it parallelizes development by enabling small autonomous teams to develop, deploy and scale their respective services independently. First option with microservices is to have a VM for each microservice. For eg, if our app has three microservices : frontend, backend and db then each microservice will run on a seperate VM. However, a virtual machine has a substantial overhead, like performance is poorer, it consumes a lot of memory in RAM & it occupies a lot of hard disk space. The idea solution would be a lightweight alternative to virtualization which brings us to docker. Hence for our application, each microservice will be containerized with docker. Instead of having the entire operating system within a docker container, docker makes sure there's only enough kernel pieces to run the corresponding microservice. Our web application will have seperate microservices for front end, database and each of the features of the backend. For example, for detecting overspeeding vehicles in a video, we will have a seperate microservice running which will take input as video and return output as snapshots of overspeeding cars. While deploying the web application, it's necessary to have container orchestration. Container orchestration is all about managing the life cycles of containers, especially in large, dynamic environments. Software teams use container orchestration to control and automate many tasks including provisioning and deployment of containers, redundancy and availability of containers, Scaling up or removing containers to spread application load evenly across host infrastructure, Movement of containers from one host to another if there is a shortage of resources in a host, or if a host dies, Allocation of resources between containers, etc. Tools Front End: We chose Angular as the front end framework. Angular is considered a full MVC framework because it offers strong opinions as to how your application should be structured. It also has much more functionality \u201cout-of-the-box\u201d. You don\u2019t need to decide which routing libraries to use or other such considerations. Back End: We chose Flask as the back end framework, since it provides simplicity, flexibility and fine-grained control. It is un-opinionated (it lets you decide how you want to implement things). Database: PostgresSQL was chosen for of its scalability. Object Detection: YOLO : You only look once (YOLO) is a state-of-the-art, real-time object detection system. Multiple Containers Management: Docker-Compose is a tool for defining and running multi-container Docker applications. With Compose, we can use a single YAML file to configure application's services.","title":"Design of the application"},{"location":"design/#requirements","text":"Automatically detect the traffic violations of rules being considered: Compliance with traffic lights Not stopping while pedestrians cross the road Compliance with speed limit Detection of a crash Violation detection should be done through the traffic data that is already available From Video data streamed from cameras, or pre-recorded data fed by user Real-time traffic signal data, that is available in an existing server. Traffic violation detection should happen automatically for all selected junctions and snapshots are to be stored in a database server. There should be a junction/camera selection which can be configured by the traffic police/ concerned authorities. Traffic violation detection should be available for monitoring by the client. They should be able to choose a particular junction/camera feed to monitor Client should be able to feed in custom video data into the system Live processed output should be available if the user demands for the same (GUI to be built for the same) Traffic violations/car crashes should be alarmed on the monitoring displays in the control room.","title":"Requirements"},{"location":"design/#assumptions","text":"Availability of a camera stream/pre recorded data and live streams are connected to the police server. (NOTE: For this project, we will be implementing a Minimum Viable Product(MVP) only the video upload and processing, since we don\u2019t have enough hardware and time to learn and implement the product for a CCTV scenario) In the case that the vehicle details should be detected, we assume that it is done by a separate system which performs automatic number plate detection, and extracts vehicle registration data from an already existing server (this is not in our project scope). Traffic light data is available from all the junctions required as real-time values.","title":"Assumptions"},{"location":"design/#design-rationale","text":"The design is based on a distributed architecture, since traffic violation detection is a resource demanding application; Not because of the algorithms involved, but due to the volume of data to be processed. The amount of data processed depends on the following factors: Traffic camera/junctions to be monitored simultaneously. The amount of data processed depends on the day, time of the day etc., due to various traffic behaviors. Because of this, we need a system that automatically scales up as well as down as per the requirement. Also, the system should support live monitoring by a number of officers (clients), which may also increase due to certain events happening in the city. Considering all these the distributed architecture is selected. When we go for distributed architecture, the next question is: \u201cWhat is the basic architecture like?\u201d. The nodes are not geographically distributed, and the network between the nodes are high speed LAN and are very reliable. Hence, the network partition is not considered. So, only node failure is considered. Hence we need a CA system.","title":"Design Rationale"},{"location":"design/#architecture","text":"","title":"Architecture"},{"location":"design/#overall-high-level-architecture","text":"","title":"Overall High Level Architecture"},{"location":"design/#software-architecture","text":"","title":"Software Architecture"},{"location":"design/#traffic-violation-engine-architecture","text":"","title":"Traffic Violation Engine Architecture"},{"location":"design/#use-case-diagram","text":"","title":"Use Case Diagram"},{"location":"design/#microservices-architecture","text":"Instead of having a monolithic architecture with layers stacked on one another, we are designing our application based on microservices architecture. Monolithic architecture only allows for vertical scaling, but with microservices architecture it's possible to achieve horizontal scaling easily. Numerous benefits of Microservices architecture include : Modularity : This makes the application easier to understand, develop, test, and become more resilient to architecture erosion. Scalability : Since microservices are implemented and deployed independently of each other, i.e. they run within independent processes, they can be monitored and scaled independently. Distributed development: it parallelizes development by enabling small autonomous teams to develop, deploy and scale their respective services independently. First option with microservices is to have a VM for each microservice. For eg, if our app has three microservices : frontend, backend and db then each microservice will run on a seperate VM. However, a virtual machine has a substantial overhead, like performance is poorer, it consumes a lot of memory in RAM & it occupies a lot of hard disk space. The idea solution would be a lightweight alternative to virtualization which brings us to docker. Hence for our application, each microservice will be containerized with docker. Instead of having the entire operating system within a docker container, docker makes sure there's only enough kernel pieces to run the corresponding microservice. Our web application will have seperate microservices for front end, database and each of the features of the backend. For example, for detecting overspeeding vehicles in a video, we will have a seperate microservice running which will take input as video and return output as snapshots of overspeeding cars. While deploying the web application, it's necessary to have container orchestration. Container orchestration is all about managing the life cycles of containers, especially in large, dynamic environments. Software teams use container orchestration to control and automate many tasks including provisioning and deployment of containers, redundancy and availability of containers, Scaling up or removing containers to spread application load evenly across host infrastructure, Movement of containers from one host to another if there is a shortage of resources in a host, or if a host dies, Allocation of resources between containers, etc.","title":"Microservices Architecture"},{"location":"design/#tools","text":"Front End: We chose Angular as the front end framework. Angular is considered a full MVC framework because it offers strong opinions as to how your application should be structured. It also has much more functionality \u201cout-of-the-box\u201d. You don\u2019t need to decide which routing libraries to use or other such considerations. Back End: We chose Flask as the back end framework, since it provides simplicity, flexibility and fine-grained control. It is un-opinionated (it lets you decide how you want to implement things). Database: PostgresSQL was chosen for of its scalability. Object Detection: YOLO : You only look once (YOLO) is a state-of-the-art, real-time object detection system. Multiple Containers Management: Docker-Compose is a tool for defining and running multi-container Docker applications. With Compose, we can use a single YAML file to configure application's services.","title":"Tools"},{"location":"docs/","text":"Documentation Server Overview Documentation server is independently hosted in a seperate Docker container. MkDocs framework is used for documentation. Changing Theme Edit theme in mkdocs.yml . Possible options are : material : Current readthedocs mkdocs Adding new sections Edit nav section in mkdocs.yml to add new sections. For each new section, you need to add corresponding markdown file.","title":"MkDocs Server"},{"location":"docs/#documentation-server","text":"","title":"Documentation Server"},{"location":"docs/#overview","text":"Documentation server is independently hosted in a seperate Docker container. MkDocs framework is used for documentation.","title":"Overview"},{"location":"docs/#changing-theme","text":"Edit theme in mkdocs.yml . Possible options are : material : Current readthedocs mkdocs","title":"Changing Theme"},{"location":"docs/#adding-new-sections","text":"Edit nav section in mkdocs.yml to add new sections. For each new section, you need to add corresponding markdown file.","title":"Adding new sections"},{"location":"features/","text":"Backend Features There are 4 submodules corresponding to different traffic violation detection. Car Crash Detection Overspeeding Detection Pedestrians Detection Traffic Signal Detection Car Crash Detection Overview This module finds crashes / accidents among Cars, Trucks, Bus, Person. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of crashes into another folder. Objectives Identify crashes between various objects. Identify the objects involved in crashes and highlight them. Store the screenshots of frames that have crashes in them. Class CrashDetectionModule Input : List of frames Function : Detects and stores crashes for a given video Functions process_rule() : Input : None Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects crash. Output : None detect_crash(current_frame, boxes, box_labels) : Input : current_frame(frame object) to be processed along with the bounding boxes (list of list) present and their box_labels (list) Trigger : Called by process_rule() Function : Main crash computation function. Output : None get_crash_objects_list(box_labels) : Input : box_labels(list) - labels of the detected objects Trigger : Called by detect_crash(...) Function : Saves the indexes of all the objects whose crash we are interested in. Output : crashObjects(list) get_overlap_variables(box) : Input : box(list) - top left and bottom right coordinates of the object Trigger : Called by detect_crash(...) Function : Computes the coordinates of the center of object and it's height and width. Output : (list) of coordinates of center and objects bounding box's height and width. detect_overlap(object1_carsh_variables, object2_crash_variables) : Input : object1_carsh_variables and object2_crash_variables list returned by the get_overlap_variables(...) function Trigger : Called by detect_crash(...) Function : Computes if there is a collision between two objects. Output : (boolean) to indicate if there is a collision or not report_crash(frame, crashing_objects) : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in crash Trigger : Called by detect_crash(...) when detect_overlap(...) return True Function : Driver function that increments the counts of total crashes call the box drawing function and the save_screenshot(...) function. Output : None draw_crashing_objects(frame, crashing_objects) : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in crash Trigger : Called by report_crash(...) Function : Highlights the objects whose crash takes place Output : frame(frame object) the modified frame which has the highlighted crashing objects add_snapshot(frame) : Input : frame(frame object) is the current frame Trigger : Called by detect_crash(...) Function : Increments the total number of crashes and stores the crash frame Output : None output_snapshot() : Input : None Trigger : - Function : Saves the screenshots of crashes into a folder Output : None Variables total_crashes : Type : Int Purpose : Stores the total number of crashes for the current frame Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame whose screentshot is saved Default Value : -1 objects_to_monitor : Type : List Purpose : Stores the index of all the objects whose crash we are interested in. (Indexes are stored according to the coco.names file) Default Value : [0, 2, 5, 7] NOTE : Refer coco.names file to add more objects inverse_collision_relaxation_x : Type : Float Purpose : One minus this fraction allows how much one object can be inside another object along x and still collision will be detected Default Value : 0.9 inverse_collision_relaxation_y : Type : Float Purpose : One minus this fraction allows how much one object can be inside another object along y and still collision will be detected Default Value : 0.6 input_video_data : Type : List Purpose : Contains the frames of the input video output_video_data : Type : List Purpose : Contains the modified frames Default Value : [] Modules Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with thier labels for crash detection. opencv : Purpose : Used for highlighting objects Workflow and Logic init () Initializes the variables then process_rule() takes input video data and then for each frame detects crash. detect_crash() then takes current frame, it\u2019s detected objects and those object labels. First it finds out all the objects whose accident/crash we are interested in using the get_crash_objects_list() function. Ex car, person, bus etc. Next it gets the coordinates which are used for collision detection from the get_overlap_variables() function. After getting these detect_overlap() checks if there is a collision. If there is the objects are passed onto the function report_crash() which in turn calls the rectangle drawing function draw_crashing_objects() and the save_screenshot() function to store the frame\u2019s screenshot. Overspeeding Overview The overspeeding module checks if there are any cars that are overspeeding on the marked lane Objectives To detect overspeeding cars on the lane To save screenshots of the overspeeding cars Functions process_rule : Input : frames of video Trigger : External trigger from composite object Function : Tracks cars in the lane and checks the speed by calculating the time taken to cover the lane distance Output : None, just calls relevant functions to perform actions in case of overspeeding check_line_intersection : Input : two lines, which are each a tuple of tuples Trigger : called for the process_rule to see if a car has crossed the start/end line Function : to find if the two lines have intersected Output : returns a boolean value depending on whether the lines have intersected or not calculate_speed : Input : number of frames taken for a bounding box to cross the lane Trigger : called to check speed of the car Function : to calculate the actual speed of the car Output : returns the speed of the vehicle add_snapshot : Input : current frame, the boundng box coordinates of the car, speed of the car, and current frame number Trigger : called when the car exceeds the speed limit Function : to add snapshot details of a violating car to a list Output : appends a snapshot to the list output_snapshots : Input : none Trigger : Called from external module Function : saves the snapshots taken Output : Saves snapshots in the format speed_timeofvideo_randomstring.jpg Variables The generic metadata have been loaded as variables, as a start and end line for measuring speed, a tracker to keep track of the SORT results, and a snapshoot details array. Logic The overspeed detection module is one of the traffic rule violation detection modules. The user marks the zone in which the over speed detection should take place \u2013 by marking a rectangle in the front end. This sends back the co-ordinates of the edges of the zone to this module. Also the user enters the road length for the marked speed detection zone. This module uses the object detection algorithm and the SORT Algorithm to detect and track objects across the frames of the video (in future CCTV frames). Once the object detected (box) crosses the start and end edges of the speed detection zone the speed of the object is calculated using the following formula: 3.6 * frame rate * road distamce/ frames taken by the car to cross the lines If the speed is within the speed limit, no action is taken. Once the speed exceeds the speed limit entered by the user, the object is marked in RED and the screenshot is saved in the prescribed folder. Modules Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame opencv : Purpose : Used for highlighting objects SORT : Purpose: to track the object across frames Yield for Pedestrians Detection Overview This module finds Cars, Trucks, Buses or Motorcycles which enter Pedestrian area while it's still in use for crossing. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of violations into another folder. Objectives Identify vehicles which violate Pedestrian Crossing. Store the screenshots of frames that have violations in them. Functions process_rule : Input : None Trigger : External trigger from composite object Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects any violations. Output : None draw_pedestrian_boxes : Input : frame(frame object) to be processed along with the waiting_boxes (list of list) and crossing_boxes(List of List) obtained from assign_pedestrian_boxes() Trigger : Called by process_rule() Function : Highlights the waiting and crossing areas Output : frame(frame object) detect_violation : Input : frame(frame object) to be processed along with the bounding boxes (tuple of list of list) present and their labels (list) Trigger : Called by process_rule() Function : Detects if a vehicle does not stop when pedestrians are waiting to cross the road or are already crossing the road Output : None object_in_box : Input : obj(list) contains coordinates of the the object and box(list) contains coordinates of the box Trigger : Called by detect_violation() Function : Detects if the object is inside the box Output : (boolean) to indicate if the object is inside the box or not draw_violating_objects : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in violation (vehicle and person) Trigger : Called by detect_violation() Function : Highlights the vehicles that violate rules Output : frame(frame object) the modified frame which has the highlighted violating objects add_snapshot : Input : frame(frame object) is the current frame Trigger : Called by detect_violation() Function : Saves a screenshot of the violation frame Output : None output_snapshots : Input : None Trigger : External trigger from composite object Function : Saves the screenshots of violations into a folder Output : None set_metadata(metadata) : Input : metadata (dict) Trigger : External trigger from composite object Function : Loads the meta data Output : None Variables num_violations : Type : Int Purpose : Stores the total number of violations in the video Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame currently being processed Default Value : -1 waiting_boxes : Type : List of List Purpose : Stores the coordinates of the pedestrian waiting areas Default Value : [] crossing_boxes : Type : List of List Purpose : Stores the coordinates of the zebra crossing areas Default Value : [] vehicle_list : Type : List Purpose : Stores names of vehicle whose violations need to be kept track of Default Value : ['motorbike', 'bus', 'truck', 'car'] input_video_data : Type : List Purpose : Contains the frames of the input video Default Value: [] output_video_data : Type : List Purpose : Contains the modified frames Default Value : [] Modules Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with their labels for crash detection. opencv : Purpose : Used for highlighting objects Logic We check if there is a pedestrian either waiting to cross the road or already crossing the road. Next we check if any vehicles do not stop for the pedestrians to cross the road. This is done by checking if a given object is in the box area defined as pedestrian area. If there is a violation, the vehicles violating the rule are highlighted and a screenshot is saved. Stop for Traffic Signal Detection Overview This module finds Cars, Trucks, Buses or Motorcycles which break Traffic Signals. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of violations into another folder. Objectives Identify vehicles which violate Traffic Signals. Store the screenshots of frames that have violations in them. Functions process_rule : Input : None Trigger : External trigger from composite object Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects any violations. Output : None simulate_signal : Input : None Trigger : process_rule() Function : Simulates a traffic signal Output : signal(list) draw_traffic_box(frame) : Input : frame(frame object) to be processed Trigger : Called by process_rule() Function : Highlights the traffic stop area for signals Output : frame(frame object) detect_violation : Input : frame(frame object) to be processed along with the bounding boxes (list of list) present and their labels (list) Trigger : Called by process_rule() Function : Detects if there is a vehicle outside the traffic box Output : None object_in_box : Input : obj(list) contains coordinates of the the object and box(list) contains coordinates of the box Trigger : Called by detect_violation() Function : Detects if the object is inside the box Output : (boolean) to indicate if the object is inside the box or not draw_violating_objects(frame, violating_objects) : Input : frame(frame object) is the current frame and violating_objects(list of list) consists of the coordinates of the bounding boxes of vehicles that come out of the traffic box Trigger : Called by detect_violation() Function : Highlights the vehicles that violate rules Output : frame(frame object) the modified frame which has the highlighted violating objects add_snapshot(frame) : Input : frame(frame object) is the current frame Trigger : Called by detect_violation() Function : Increments the total number of violations and stores the violation frame Output : None output_snapshot() : Input : None Trigger : External trigger from composite object Function : Saves the screenshots of violations into a folder Output : None set_metadata(metadata) : Input : metadata (dict) Trigger : External trigger from composite object Function : Loads the meta data Output : None Variables num_violations : Type : Int Purpose : Stores the total number of violations for the current frame Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame whose screentshot is saved Default Value : -1 traffic_box : Type : List Purpose : Stores the coordinates of the traffic box Default Value : [] crossing_boxes : Type : List of List Purpose : Stores the coordinates of the crossing areas Default Value : [] vehicle_list : Type : List Purpose : Stores names of vehicle whose violations need to be kept track of Default Value : ['motorbike', 'bus', 'truck', 'car'] input_video_data : Type : List Purpose : Contains the frames of the input video Default Value : [] output_video_data : Type : List Purpose : Contains the modified frames Default Value : [] Modules Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with thier labels for crash detection. opencv : Purpose : Used for highlighting objects Logic We keep track of the all the vehicles coordinates and if it steps out traffic box then we detect a violation. For this, we check if the rectangle defining a vehicle and the traffic box are overlapping.","title":"Backend Features"},{"location":"features/#backend-features","text":"There are 4 submodules corresponding to different traffic violation detection. Car Crash Detection Overspeeding Detection Pedestrians Detection Traffic Signal Detection","title":"Backend Features"},{"location":"features/#car-crash-detection","text":"","title":"Car Crash Detection"},{"location":"features/#overview","text":"This module finds crashes / accidents among Cars, Trucks, Bus, Person. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of crashes into another folder.","title":"Overview"},{"location":"features/#objectives","text":"Identify crashes between various objects. Identify the objects involved in crashes and highlight them. Store the screenshots of frames that have crashes in them.","title":"Objectives"},{"location":"features/#class","text":"CrashDetectionModule Input : List of frames Function : Detects and stores crashes for a given video","title":"Class"},{"location":"features/#functions","text":"process_rule() : Input : None Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects crash. Output : None detect_crash(current_frame, boxes, box_labels) : Input : current_frame(frame object) to be processed along with the bounding boxes (list of list) present and their box_labels (list) Trigger : Called by process_rule() Function : Main crash computation function. Output : None get_crash_objects_list(box_labels) : Input : box_labels(list) - labels of the detected objects Trigger : Called by detect_crash(...) Function : Saves the indexes of all the objects whose crash we are interested in. Output : crashObjects(list) get_overlap_variables(box) : Input : box(list) - top left and bottom right coordinates of the object Trigger : Called by detect_crash(...) Function : Computes the coordinates of the center of object and it's height and width. Output : (list) of coordinates of center and objects bounding box's height and width. detect_overlap(object1_carsh_variables, object2_crash_variables) : Input : object1_carsh_variables and object2_crash_variables list returned by the get_overlap_variables(...) function Trigger : Called by detect_crash(...) Function : Computes if there is a collision between two objects. Output : (boolean) to indicate if there is a collision or not report_crash(frame, crashing_objects) : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in crash Trigger : Called by detect_crash(...) when detect_overlap(...) return True Function : Driver function that increments the counts of total crashes call the box drawing function and the save_screenshot(...) function. Output : None draw_crashing_objects(frame, crashing_objects) : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in crash Trigger : Called by report_crash(...) Function : Highlights the objects whose crash takes place Output : frame(frame object) the modified frame which has the highlighted crashing objects add_snapshot(frame) : Input : frame(frame object) is the current frame Trigger : Called by detect_crash(...) Function : Increments the total number of crashes and stores the crash frame Output : None output_snapshot() : Input : None Trigger : - Function : Saves the screenshots of crashes into a folder Output : None","title":"Functions"},{"location":"features/#variables","text":"total_crashes : Type : Int Purpose : Stores the total number of crashes for the current frame Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame whose screentshot is saved Default Value : -1 objects_to_monitor : Type : List Purpose : Stores the index of all the objects whose crash we are interested in. (Indexes are stored according to the coco.names file) Default Value : [0, 2, 5, 7] NOTE : Refer coco.names file to add more objects inverse_collision_relaxation_x : Type : Float Purpose : One minus this fraction allows how much one object can be inside another object along x and still collision will be detected Default Value : 0.9 inverse_collision_relaxation_y : Type : Float Purpose : One minus this fraction allows how much one object can be inside another object along y and still collision will be detected Default Value : 0.6 input_video_data : Type : List Purpose : Contains the frames of the input video output_video_data : Type : List Purpose : Contains the modified frames Default Value : []","title":"Variables"},{"location":"features/#modules","text":"Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with thier labels for crash detection. opencv : Purpose : Used for highlighting objects","title":"Modules"},{"location":"features/#workflow-and-logic","text":"init () Initializes the variables then process_rule() takes input video data and then for each frame detects crash. detect_crash() then takes current frame, it\u2019s detected objects and those object labels. First it finds out all the objects whose accident/crash we are interested in using the get_crash_objects_list() function. Ex car, person, bus etc. Next it gets the coordinates which are used for collision detection from the get_overlap_variables() function. After getting these detect_overlap() checks if there is a collision. If there is the objects are passed onto the function report_crash() which in turn calls the rectangle drawing function draw_crashing_objects() and the save_screenshot() function to store the frame\u2019s screenshot.","title":"Workflow and Logic"},{"location":"features/#overspeeding","text":"","title":"Overspeeding"},{"location":"features/#overview_1","text":"The overspeeding module checks if there are any cars that are overspeeding on the marked lane","title":"Overview"},{"location":"features/#objectives_1","text":"To detect overspeeding cars on the lane To save screenshots of the overspeeding cars","title":"Objectives"},{"location":"features/#functions_1","text":"process_rule : Input : frames of video Trigger : External trigger from composite object Function : Tracks cars in the lane and checks the speed by calculating the time taken to cover the lane distance Output : None, just calls relevant functions to perform actions in case of overspeeding check_line_intersection : Input : two lines, which are each a tuple of tuples Trigger : called for the process_rule to see if a car has crossed the start/end line Function : to find if the two lines have intersected Output : returns a boolean value depending on whether the lines have intersected or not calculate_speed : Input : number of frames taken for a bounding box to cross the lane Trigger : called to check speed of the car Function : to calculate the actual speed of the car Output : returns the speed of the vehicle add_snapshot : Input : current frame, the boundng box coordinates of the car, speed of the car, and current frame number Trigger : called when the car exceeds the speed limit Function : to add snapshot details of a violating car to a list Output : appends a snapshot to the list output_snapshots : Input : none Trigger : Called from external module Function : saves the snapshots taken Output : Saves snapshots in the format speed_timeofvideo_randomstring.jpg","title":"Functions"},{"location":"features/#variables_1","text":"The generic metadata have been loaded as variables, as a start and end line for measuring speed, a tracker to keep track of the SORT results, and a snapshoot details array.","title":"Variables"},{"location":"features/#logic","text":"The overspeed detection module is one of the traffic rule violation detection modules. The user marks the zone in which the over speed detection should take place \u2013 by marking a rectangle in the front end. This sends back the co-ordinates of the edges of the zone to this module. Also the user enters the road length for the marked speed detection zone. This module uses the object detection algorithm and the SORT Algorithm to detect and track objects across the frames of the video (in future CCTV frames). Once the object detected (box) crosses the start and end edges of the speed detection zone the speed of the object is calculated using the following formula: 3.6 * frame rate * road distamce/ frames taken by the car to cross the lines If the speed is within the speed limit, no action is taken. Once the speed exceeds the speed limit entered by the user, the object is marked in RED and the screenshot is saved in the prescribed folder.","title":"Logic"},{"location":"features/#modules_1","text":"Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame opencv : Purpose : Used for highlighting objects SORT : Purpose: to track the object across frames","title":"Modules"},{"location":"features/#yield-for-pedestrians-detection","text":"","title":"Yield for Pedestrians Detection"},{"location":"features/#overview_2","text":"This module finds Cars, Trucks, Buses or Motorcycles which enter Pedestrian area while it's still in use for crossing. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of violations into another folder.","title":"Overview"},{"location":"features/#objectives_2","text":"Identify vehicles which violate Pedestrian Crossing. Store the screenshots of frames that have violations in them.","title":"Objectives"},{"location":"features/#functions_2","text":"process_rule : Input : None Trigger : External trigger from composite object Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects any violations. Output : None draw_pedestrian_boxes : Input : frame(frame object) to be processed along with the waiting_boxes (list of list) and crossing_boxes(List of List) obtained from assign_pedestrian_boxes() Trigger : Called by process_rule() Function : Highlights the waiting and crossing areas Output : frame(frame object) detect_violation : Input : frame(frame object) to be processed along with the bounding boxes (tuple of list of list) present and their labels (list) Trigger : Called by process_rule() Function : Detects if a vehicle does not stop when pedestrians are waiting to cross the road or are already crossing the road Output : None object_in_box : Input : obj(list) contains coordinates of the the object and box(list) contains coordinates of the box Trigger : Called by detect_violation() Function : Detects if the object is inside the box Output : (boolean) to indicate if the object is inside the box or not draw_violating_objects : Input : frame(frame object) is the current frame and crashing_objects(list of list) consists of the coordinates of the bounding boxes of the two objects involved in violation (vehicle and person) Trigger : Called by detect_violation() Function : Highlights the vehicles that violate rules Output : frame(frame object) the modified frame which has the highlighted violating objects add_snapshot : Input : frame(frame object) is the current frame Trigger : Called by detect_violation() Function : Saves a screenshot of the violation frame Output : None output_snapshots : Input : None Trigger : External trigger from composite object Function : Saves the screenshots of violations into a folder Output : None set_metadata(metadata) : Input : metadata (dict) Trigger : External trigger from composite object Function : Loads the meta data Output : None","title":"Functions"},{"location":"features/#variables_2","text":"num_violations : Type : Int Purpose : Stores the total number of violations in the video Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame currently being processed Default Value : -1 waiting_boxes : Type : List of List Purpose : Stores the coordinates of the pedestrian waiting areas Default Value : [] crossing_boxes : Type : List of List Purpose : Stores the coordinates of the zebra crossing areas Default Value : [] vehicle_list : Type : List Purpose : Stores names of vehicle whose violations need to be kept track of Default Value : ['motorbike', 'bus', 'truck', 'car'] input_video_data : Type : List Purpose : Contains the frames of the input video Default Value: [] output_video_data : Type : List Purpose : Contains the modified frames Default Value : []","title":"Variables"},{"location":"features/#modules_2","text":"Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with their labels for crash detection. opencv : Purpose : Used for highlighting objects","title":"Modules"},{"location":"features/#logic_1","text":"We check if there is a pedestrian either waiting to cross the road or already crossing the road. Next we check if any vehicles do not stop for the pedestrians to cross the road. This is done by checking if a given object is in the box area defined as pedestrian area. If there is a violation, the vehicles violating the rule are highlighted and a screenshot is saved.","title":"Logic"},{"location":"features/#stop-for-traffic-signal-detection","text":"","title":"Stop for Traffic Signal Detection"},{"location":"features/#overview_3","text":"This module finds Cars, Trucks, Buses or Motorcycles which break Traffic Signals. Class takes a list of frames passed on from the Object Detection Module as input and saves screenshot of violations into another folder.","title":"Overview"},{"location":"features/#objectives_3","text":"Identify vehicles which violate Traffic Signals. Store the screenshots of frames that have violations in them.","title":"Objectives"},{"location":"features/#functions_3","text":"process_rule : Input : None Trigger : External trigger from composite object Function : Driver function having all major function calls. Takes each frame, sends it to object detection module and then detects any violations. Output : None simulate_signal : Input : None Trigger : process_rule() Function : Simulates a traffic signal Output : signal(list) draw_traffic_box(frame) : Input : frame(frame object) to be processed Trigger : Called by process_rule() Function : Highlights the traffic stop area for signals Output : frame(frame object) detect_violation : Input : frame(frame object) to be processed along with the bounding boxes (list of list) present and their labels (list) Trigger : Called by process_rule() Function : Detects if there is a vehicle outside the traffic box Output : None object_in_box : Input : obj(list) contains coordinates of the the object and box(list) contains coordinates of the box Trigger : Called by detect_violation() Function : Detects if the object is inside the box Output : (boolean) to indicate if the object is inside the box or not draw_violating_objects(frame, violating_objects) : Input : frame(frame object) is the current frame and violating_objects(list of list) consists of the coordinates of the bounding boxes of vehicles that come out of the traffic box Trigger : Called by detect_violation() Function : Highlights the vehicles that violate rules Output : frame(frame object) the modified frame which has the highlighted violating objects add_snapshot(frame) : Input : frame(frame object) is the current frame Trigger : Called by detect_violation() Function : Increments the total number of violations and stores the violation frame Output : None output_snapshot() : Input : None Trigger : External trigger from composite object Function : Saves the screenshots of violations into a folder Output : None set_metadata(metadata) : Input : metadata (dict) Trigger : External trigger from composite object Function : Loads the meta data Output : None","title":"Functions"},{"location":"features/#variables_3","text":"num_violations : Type : Int Purpose : Stores the total number of violations for the current frame Default Value : 0 frame_saved : Type : Int Purpose : Frame number of the last frame whose screentshot was saved Default Value : -100 current_frame_num : Type : Int Purpose : Frame number of the frame whose screentshot is saved Default Value : -1 traffic_box : Type : List Purpose : Stores the coordinates of the traffic box Default Value : [] crossing_boxes : Type : List of List Purpose : Stores the coordinates of the crossing areas Default Value : [] vehicle_list : Type : List Purpose : Stores names of vehicle whose violations need to be kept track of Default Value : ['motorbike', 'bus', 'truck', 'car'] input_video_data : Type : List Purpose : Contains the frames of the input video Default Value : [] output_video_data : Type : List Purpose : Contains the modified frames Default Value : []","title":"Variables"},{"location":"features/#modules_3","text":"Following is the list of modules used for this feature. objectDetectionModule : Purpose : Returns all the objects in the frame along with thier labels for crash detection. opencv : Purpose : Used for highlighting objects","title":"Modules"},{"location":"features/#logic_2","text":"We keep track of the all the vehicles coordinates and if it steps out traffic box then we detect a violation. For this, we check if the rectangle defining a vehicle and the traffic box are overlapping.","title":"Logic"},{"location":"frontend/","text":"Frontend Overview Angular ( TypeScript ) framework is used for frontend of the application. We have written code in a modular way, segregating different parts into following components. App root component Input Video component Video Streaming component Snapshots Rendering component Radio buttons component App root component Main component of the framework. Objectives Keep Track of state of the application. Manage global state of the application. Functions refreshEvent(event) : Input : Feature Type wrapped in an Angular Event Trigger : User clicking a different feature type will trigger this function Function : Reloading video stream and snapshots based on selected feature Output : None stateOff(event) : Input : Application state wrapped in an Angular Event Trigger : Backend processing getting completed Function : Changing application state at global level Output : None Variables state : Type : Boolean ( True / False ) Purpose : Global application state indicating whether media files should be rendered or not Default Value : false title : Type : String Purpose : Title of the application Default Value : frontend Modules @angular/core/ViewChild : Purpose : To access video streaming and snapshots components from typescript code. Input Video Component Objectives Take user's video as input and send it to backend Once user selects video, ask users for different parameters which will be used in analysis Send parameters to backend for analysis Functions startProcessing() : Input : None Trigger : User clicking 'Start' button Purpose : To set the application status to 'processing' Output : None finishProcessing() : Input : None Trigger : Backend finishing processing Purpose : To set the application status to done with processing Output : None upload() : Input : None Trigger : User clicking upload button Purpose : To upload video to backend Output : None getImage() : Input : Image url ( String ) Trigger : User starting process of uploaded video Purpose : To get a frame of video on which user can draw Output : None openModal(templateReference) : Input : Reference to a modal box tag in template Trigger : User clicking next on modal boxes Purpose : Open the corresponding modal box Output : None saveRoadLength(input_) : Input : Reference to input tag in template Trigger : User clicking next after typing road length Purpose : Save the typed road length input Output : None loadCanvas(modalName) : Input : Name of modal box Trigger : User wanting to draw on image Purpose : Load canvas on top of image on which user can draw Output : None btnClick() : Input : None Trigger : Finishing inputting parameters Purpose : Finishing inputting parameters Output : None finalize() : Input : None Trigger : User clicking 'Next' button in the last modal box Purpose : Finalize parameters Output : None makePolygon() : Input : None Trigger : User clicking 4th point in the drawing Purpose : Making polygon out of selected points Output : None clearCanvas() : Input : None Trigger : User finishing drawing Purpose : Clear drawing canvas Output : None diableButtonsOnClearCanvas() : Input : None Trigger : User clicking buttons on canvas Purpose : Disable buttons on clear canvas selection Output : None getCoordinatesZebra() : Input : None Trigger : User clicking next on zebra crossing modal box Purpose : To save coordinates of zebra corssing Output : None getCoordinatesPedestrian() : Input : None Trigger : User clicking next on pedestrian modal box Purpose : To save coordinates of pedesitrian area Output : None getCoordinatesLane() : Input : None Trigger : User clicking next on lane marking modal box Purpose : To save coordinates of lane markings Output : None Variables uploadedFiles : Type : Array<'File'> Purpose : Store uploaded files Default Value : Empty state : Type : Boolean Purpose : State of the application Default Value : Same as root component processingStatus : Type : Boolean Purpose : Stores whether processing is completed or not Default Value : false stateOff : Type : Event Emitter Purpose : To trigger parent component to change status Default Value : false uploadStatus : Type : Boolean Purpose : To show whether uploading is done or not Default Value : false scalingFactorX : Type : Float Purpose : To show how much to scale coordinates in x direction Default Value : 1 scalingFactorY : Type : Float Purpose : To show how much to scale coordinates in y direction Default Value : 1 uploaded : Type : Boolean Purpose : To show status about uploading Default Value : false imageToShow : Type : any Purpose : Image which will be showed Default Value : None isImageLoading : Type : any Purpose : To store status of image loading Default Value : None fileName : Type : String Purpose : To store filename of uploaded video Default Value : Empty String uploadDisabled : Type : Boolean Purpose : To show whether upload button is disabled or not Default value : false fileSize : Type : Integer Purpose : To store size of file Default value : 0 inputSpeedForm : Type : FormGroup Purpose : Form for taking speed limit as input Default value : None roadLendthForm : Type : FormGroup Purpose : Form for taking road length as input Default value : None speedlimit : Type : Integer Purpose : Speedlimit Default Value : 100 roadLendth : Type : Integer Purpose : To store approximate lenght of road in metres Default Value : 0 Modules @angular/common/http : Purpose : To make HTTP requests including posting videos and parameters rxjs/operators : Purpose : To extend timeout period of HTTP requests to ensure that Video Streaming Component Objectives Render video from backend onto frontend Reload appropriate video based on feature selected Functions reload(feature) : Input : Rule breaking feature selected Trigger : User clicking radio button of a traffic rule feature Purpose : To change the video based on traffic rule selected Output : None Variables videoelement : Type : HTMLVideoElement Purpose : Based on feature selected, reloading video Default Value : None base : Type : String Purpose : Base url from where to retrieve video Default Value : '/api/getvideo/' url : Type : String Purpose : Final url from where to retrieve video Default Value : '/api/getvideo/' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render application or not Default Value : Same as Parent Modules @angular/core/Input : Purpose : To bind a variable to parent component variable Snapshots Rendering Component Objectives Retrieve images corresponding to a particular feature Render images with a scrollbar Functions reload(feature) : Input : Rule breaking feature selected Trigger : User clicking radio button of a traffic rule feature Purpose : To change the images based on traffic rule selected Output : None Variables imagesList : Type : Array Purpose : To store list of images to render on the component Default Value : Empty Array feature : Type : String Purpose : Global variable to store feature, corresponding to that feature images will be rendered Default Value : 'default' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render images or not Default Value : Same as Parent Modules @angular/common/http : Purpose : To make HTTP request to retrieve images from backend Radio buttons component Objectives Provide user with radio buttons for easy navigation between different features Based on radio button selected reload video streaming and rendering snapshots Functions handleClick(event) : Input : Rule breaking feature selected wrapped in an event Trigger : User clicking radio button of a traffic rule feature Purpose : To invoke reload functions in snapshot and video components Output : None Variables refreshEvent : Type : EventEmitter Purpose : Event Emitter, binded with parent Default Value : None feature : Type : String Purpose : Global variable to store feature, that feature will be passed to snapshots and video components Default Value : 'default' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render images/video or not Default Value : Same as Parent Modules @angular/common/http : Purpose : To make HTTP request to retrieve images from backend @angular/core/EventEmitter : Purpose : Binding variables from parent to child components","title":"Frontend Service"},{"location":"frontend/#frontend","text":"","title":"Frontend"},{"location":"frontend/#overview","text":"Angular ( TypeScript ) framework is used for frontend of the application. We have written code in a modular way, segregating different parts into following components. App root component Input Video component Video Streaming component Snapshots Rendering component Radio buttons component","title":"Overview"},{"location":"frontend/#app-root-component","text":"Main component of the framework.","title":"App root component"},{"location":"frontend/#objectives","text":"Keep Track of state of the application. Manage global state of the application.","title":"Objectives"},{"location":"frontend/#functions","text":"refreshEvent(event) : Input : Feature Type wrapped in an Angular Event Trigger : User clicking a different feature type will trigger this function Function : Reloading video stream and snapshots based on selected feature Output : None stateOff(event) : Input : Application state wrapped in an Angular Event Trigger : Backend processing getting completed Function : Changing application state at global level Output : None","title":"Functions"},{"location":"frontend/#variables","text":"state : Type : Boolean ( True / False ) Purpose : Global application state indicating whether media files should be rendered or not Default Value : false title : Type : String Purpose : Title of the application Default Value : frontend","title":"Variables"},{"location":"frontend/#modules","text":"@angular/core/ViewChild : Purpose : To access video streaming and snapshots components from typescript code.","title":"Modules"},{"location":"frontend/#input-video-component","text":"","title":"Input Video Component"},{"location":"frontend/#objectives_1","text":"Take user's video as input and send it to backend Once user selects video, ask users for different parameters which will be used in analysis Send parameters to backend for analysis","title":"Objectives"},{"location":"frontend/#functions_1","text":"startProcessing() : Input : None Trigger : User clicking 'Start' button Purpose : To set the application status to 'processing' Output : None finishProcessing() : Input : None Trigger : Backend finishing processing Purpose : To set the application status to done with processing Output : None upload() : Input : None Trigger : User clicking upload button Purpose : To upload video to backend Output : None getImage() : Input : Image url ( String ) Trigger : User starting process of uploaded video Purpose : To get a frame of video on which user can draw Output : None openModal(templateReference) : Input : Reference to a modal box tag in template Trigger : User clicking next on modal boxes Purpose : Open the corresponding modal box Output : None saveRoadLength(input_) : Input : Reference to input tag in template Trigger : User clicking next after typing road length Purpose : Save the typed road length input Output : None loadCanvas(modalName) : Input : Name of modal box Trigger : User wanting to draw on image Purpose : Load canvas on top of image on which user can draw Output : None btnClick() : Input : None Trigger : Finishing inputting parameters Purpose : Finishing inputting parameters Output : None finalize() : Input : None Trigger : User clicking 'Next' button in the last modal box Purpose : Finalize parameters Output : None makePolygon() : Input : None Trigger : User clicking 4th point in the drawing Purpose : Making polygon out of selected points Output : None clearCanvas() : Input : None Trigger : User finishing drawing Purpose : Clear drawing canvas Output : None diableButtonsOnClearCanvas() : Input : None Trigger : User clicking buttons on canvas Purpose : Disable buttons on clear canvas selection Output : None getCoordinatesZebra() : Input : None Trigger : User clicking next on zebra crossing modal box Purpose : To save coordinates of zebra corssing Output : None getCoordinatesPedestrian() : Input : None Trigger : User clicking next on pedestrian modal box Purpose : To save coordinates of pedesitrian area Output : None getCoordinatesLane() : Input : None Trigger : User clicking next on lane marking modal box Purpose : To save coordinates of lane markings Output : None","title":"Functions"},{"location":"frontend/#variables_1","text":"uploadedFiles : Type : Array<'File'> Purpose : Store uploaded files Default Value : Empty state : Type : Boolean Purpose : State of the application Default Value : Same as root component processingStatus : Type : Boolean Purpose : Stores whether processing is completed or not Default Value : false stateOff : Type : Event Emitter Purpose : To trigger parent component to change status Default Value : false uploadStatus : Type : Boolean Purpose : To show whether uploading is done or not Default Value : false scalingFactorX : Type : Float Purpose : To show how much to scale coordinates in x direction Default Value : 1 scalingFactorY : Type : Float Purpose : To show how much to scale coordinates in y direction Default Value : 1 uploaded : Type : Boolean Purpose : To show status about uploading Default Value : false imageToShow : Type : any Purpose : Image which will be showed Default Value : None isImageLoading : Type : any Purpose : To store status of image loading Default Value : None fileName : Type : String Purpose : To store filename of uploaded video Default Value : Empty String uploadDisabled : Type : Boolean Purpose : To show whether upload button is disabled or not Default value : false fileSize : Type : Integer Purpose : To store size of file Default value : 0 inputSpeedForm : Type : FormGroup Purpose : Form for taking speed limit as input Default value : None roadLendthForm : Type : FormGroup Purpose : Form for taking road length as input Default value : None speedlimit : Type : Integer Purpose : Speedlimit Default Value : 100 roadLendth : Type : Integer Purpose : To store approximate lenght of road in metres Default Value : 0","title":"Variables"},{"location":"frontend/#modules_1","text":"@angular/common/http : Purpose : To make HTTP requests including posting videos and parameters rxjs/operators : Purpose : To extend timeout period of HTTP requests to ensure that","title":"Modules"},{"location":"frontend/#video-streaming-component","text":"","title":"Video Streaming Component"},{"location":"frontend/#objectives_2","text":"Render video from backend onto frontend Reload appropriate video based on feature selected","title":"Objectives"},{"location":"frontend/#functions_2","text":"reload(feature) : Input : Rule breaking feature selected Trigger : User clicking radio button of a traffic rule feature Purpose : To change the video based on traffic rule selected Output : None","title":"Functions"},{"location":"frontend/#variables_2","text":"videoelement : Type : HTMLVideoElement Purpose : Based on feature selected, reloading video Default Value : None base : Type : String Purpose : Base url from where to retrieve video Default Value : '/api/getvideo/' url : Type : String Purpose : Final url from where to retrieve video Default Value : '/api/getvideo/' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render application or not Default Value : Same as Parent","title":"Variables"},{"location":"frontend/#modules_2","text":"@angular/core/Input : Purpose : To bind a variable to parent component variable","title":"Modules"},{"location":"frontend/#snapshots-rendering-component","text":"","title":"Snapshots Rendering Component"},{"location":"frontend/#objectives_3","text":"Retrieve images corresponding to a particular feature Render images with a scrollbar","title":"Objectives"},{"location":"frontend/#functions_3","text":"reload(feature) : Input : Rule breaking feature selected Trigger : User clicking radio button of a traffic rule feature Purpose : To change the images based on traffic rule selected Output : None","title":"Functions"},{"location":"frontend/#variables_3","text":"imagesList : Type : Array Purpose : To store list of images to render on the component Default Value : Empty Array feature : Type : String Purpose : Global variable to store feature, corresponding to that feature images will be rendered Default Value : 'default' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render images or not Default Value : Same as Parent","title":"Variables"},{"location":"frontend/#modules_3","text":"@angular/common/http : Purpose : To make HTTP request to retrieve images from backend","title":"Modules"},{"location":"frontend/#radio-buttons-component","text":"","title":"Radio buttons component"},{"location":"frontend/#objectives_4","text":"Provide user with radio buttons for easy navigation between different features Based on radio button selected reload video streaming and rendering snapshots","title":"Objectives"},{"location":"frontend/#functions_4","text":"handleClick(event) : Input : Rule breaking feature selected wrapped in an event Trigger : User clicking radio button of a traffic rule feature Purpose : To invoke reload functions in snapshot and video components Output : None","title":"Functions"},{"location":"frontend/#variables_4","text":"refreshEvent : Type : EventEmitter Purpose : Event Emitter, binded with parent Default Value : None feature : Type : String Purpose : Global variable to store feature, that feature will be passed to snapshots and video components Default Value : 'default' state : Type : Boolean Purpose : Global application state binded to parent, to show whether to render images/video or not Default Value : Same as Parent","title":"Variables"},{"location":"frontend/#modules_4","text":"@angular/common/http : Purpose : To make HTTP request to retrieve images from backend @angular/core/EventEmitter : Purpose : Binding variables from parent to child components","title":"Modules"},{"location":"main/","text":"Backend Main Module Overview The backend module processes the video and and returns video and screenshots to the frontend for display. Input .mp4 video from user Output .webm video and screenshots of traffic violations. Class Structure A CompositeRuleObject, that acts like a pseudo factory method pattern, encloses all the rules, constructs their settings, and runs them. A VideoToFrame object converts the video into a list of frames, and gives it to the CompositeRuleObject, which in turn calls each object's process_rule, output_snapshots & output_processed_frames functions. The output frames are passed through a FrameToVideo object, which converts the frames into a video.","title":"Backend Main Module"},{"location":"main/#backend-main-module","text":"","title":"Backend Main Module"},{"location":"main/#overview","text":"The backend module processes the video and and returns video and screenshots to the frontend for display.","title":"Overview"},{"location":"main/#input","text":".mp4 video from user","title":"Input"},{"location":"main/#output","text":".webm video and screenshots of traffic violations.","title":"Output"},{"location":"main/#class-structure","text":"A CompositeRuleObject, that acts like a pseudo factory method pattern, encloses all the rules, constructs their settings, and runs them. A VideoToFrame object converts the video into a list of frames, and gives it to the CompositeRuleObject, which in turn calls each object's process_rule, output_snapshots & output_processed_frames functions. The output frames are passed through a FrameToVideo object, which converts the frames into a video.","title":"Class Structure"}]}